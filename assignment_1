----------------------------------------------------------------------
Cancelled Flight Carriers
----------------------------------------
Find all carriers who cancelled more than 1 flights during 2007, order them from biggest to lowest by number 
of cancelled flights and list in each record all departure cities where cancellation happened.

Input:
carrier,FlightNum,cancel_status,dep_city,    journey_dt
AIR1 ,   A101 	  ,0,			Hyd,	  	 2010-1-1
AIR1 ,   A110 	  ,1,			Chicago,	 2007-1-1
AIR1 ,   A101 	  ,1,			New York,	 2007-10-1
AIR1 ,   A145 	  ,1,			San Diego,	 2007-05-25
AIR1 ,   A148 	  ,1,			San Diego,	 2009-05-25
AIR2 ,   E101,	  0,			Hyd,		 2010-1-1
AIR2 ,   E110 ,   1,			Miami,		 2007-1-1
AIR2 ,   E101 ,   1,			Tucson,		 2007-10-1
AIR2 ,   E145 ,   1,			Huston,		 2007-05-25
AIR2 ,   E148 ,   1,			San Diego,	 2009-05-25

output:
    |Carrier| Canceled flights|Cities|
    |-------|:---------------:|----------------------------:|
    |AIR1   |3             |Chicago, New York, San Diego |
    |AIR2   | 3              |Miami, Tucson, Huston, Boston|

====
Sample Twitter Data:

{
"id":"53627",
"user":"EPAM_BD",
"text":"#epam_rocks #epam_internal #EPAM #HACKATON ROCKS. #Love #EPAM #Hackathons.",
"place":"Hyderabad, INDIA"
}
{
"id":"53628",
"user":"EPAM_RDD",
"text":"#epam_rocks #epam_internal #EPAM #HACKATON ROCKS. #Love #EPAM #Hackathons.",
"place":"Hyderabad, INDIA"
}
{
"id":"53629",
"user":"EPAM_HDD",
"text":"#epam_rocks #epam_internal #EPAM #HACKATON ROCKS. #Love #EPAM #Hackathons.",
"place":"Hyderabad, INDIA
}
{
"id":"53630",
"user":"EPAM_BD",
"text":"#epam_rocks #epam_internal #EPAM #HACKATON ROCKS. #Love #EPAM #Hackathons.",
"place":"Hyderabad, INDIA"
}

Use Spark DF API to provide the solution. 

1) Convert to below structure
id      user    place            hashtag
53630  EPAM_BD  Hyderabad, INDIA  epam_rocks
53630  EPAM_BD  Hyderabad, INDIA  epam_internal
53630  EPAM_BD  Hyderabad, INDIA  EPAM
........


============

Sample coding task

Dataset structure (parquet/csv):
name | Designation | year | month | salary

Create Dataframe and then perform below ops


3)For each employee calculate salary rank within job designation for March, 2018 

val winSpec = Window.partitionBy($"designation").orderBy($"salary".desc)
empDf.filter($"year".eq("2018").and($"month".eq("March")))
empDf.withColumn("rank", dense_rank().over(winSpec)).show()

	Same scenario - find employee rank across all designations  for year 2019


5)add an extra column COUNTRY and values to be populated based on below logic 
	if empid <=100  then ABC
	if empid>100 and  <=200  then DEF
	else XYZ

empDf.withColumn("new_col", when($"empId".le(100), "ABC").when($"empid").gt(100).and($"empid".le(200)), "DEF").otherwise("XYZ")).show()


7) join 3 dataframes 
	DF1 - id ,age
	DF2 -  empid,loc
	DF3 -  eid,dept
Output to Display : empid, age , loc,  dept . for ALL the records of DF1

4) EMployee DF has 1M records. How to show the details of particular empids only .(without shuffling the data)



6)write code to modify the column names such that _1 is added to each column name  ie(name_1,designation_1 etc)

